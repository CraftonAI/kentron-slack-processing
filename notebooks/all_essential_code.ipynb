{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory path\n",
    "directory_path = \"../csv2/\"\n",
    "\n",
    "# Define the column names\n",
    "column_names = [\"user\", \"text\", \"type\", \"ts\"]\n",
    "\n",
    "# Create an empty dataframe\n",
    "df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "# Loop through all subdirectories starting with \"D05\"\n",
    "for subdir, dirs, files in os.walk(directory_path):\n",
    "    if subdir.startswith(directory_path + \"D05\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                # Read the csv file\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                temp_df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Check if all required columns are present\n",
    "                if all(col in temp_df.columns for col in column_names):\n",
    "                    # Append the dataframe to the main dataframe\n",
    "                    df = df.append(temp_df[column_names], ignore_index=True)\n",
    "\n",
    "df['ts'] = pd.to_datetime(df['ts'], unit='s')\n",
    "\n",
    "# Save the dataframe to a csv file\n",
    "df.to_csv(\"merged_msg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def merge_csv_files():\n",
    "    # Take user input for usernames\n",
    "    username1 = input(\"Enter username 1: \")\n",
    "    username2 = input(\"Enter username 2: \")\n",
    "\n",
    "    # Read the dms_output.csv file\n",
    "    dms_data = pd.read_csv('../processed_data/dms_output.csv')\n",
    "\n",
    "    # Filter the data based on the usernames\n",
    "    filtered_data = dms_data[(dms_data['member_1'] == username1) & (dms_data['member_2'] == username2)]\n",
    "\n",
    "    if filtered_data.empty:\n",
    "        print(\"No matching records found\")\n",
    "        return\n",
    "\n",
    "    # Get the folder name from the id column of the matched record\n",
    "    folder_name = filtered_data.iloc[0]['id']\n",
    "\n",
    "    # Search for the folder with the same name\n",
    "    folder_path = None\n",
    "    for root, dirs, files in os.walk('../csv2/'):\n",
    "        if folder_name in dirs:\n",
    "            folder_path = os.path.join(root, folder_name)\n",
    "            break\n",
    "\n",
    "    if folder_path is None:\n",
    "        print(\"Folder not found\")\n",
    "        return\n",
    "    # Merge all the CSV files in the folder\n",
    "    merged_data = pd.DataFrame()\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            df = pd.read_csv(file_path, usecols=[\"user\", \"text\", \"type\", \"ts\"])\n",
    "            df['ts'] = pd.to_datetime(df['ts'], unit='s')  # Convert ts column to human-readable timestamp\n",
    "            merged_data = pd.concat([merged_data, df])\n",
    "\n",
    "    # Save the merged data to a new CSV file\n",
    "    merged_data.to_csv('merged_conversations.csv', index=False)\n",
    "    print(\"CSV files merged successfully!\")\n",
    "\n",
    "# Usage example\n",
    "merge_csv_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide each user in the 'members' column into different columns\n",
    "members_df = channels_df['members'].str.split(',', expand=True)\n",
    "members_df = members_df.replace({',':'', '\\[':'', '\\]':'', '\\'':''}, regex=True)\n",
    "\n",
    "# Rename the columns\n",
    "members_df.columns = [f\"member_{i+1}\" for i in range(members_df.shape[1])]\n",
    "\n",
    "# Concatenate the original DataFrame with the new columns\n",
    "channels_df = pd.concat([channels_df, members_df], axis=1)\n",
    "\n",
    "# Display the modified DataFrame\n",
    "channels_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dms_df = pd.read_csv('../csv2/dms.csv')\n",
    "\n",
    "# Divide each user in the 'members' column into different columns\n",
    "members_df = dms_df['members'].str.split(',', expand=True)\n",
    "members_df = members_df.replace({',':'', '\\[':'', '\\]':'', '\\'':''}, regex=True)\n",
    "\n",
    "# Rename the columns\n",
    "members_df.columns = [f\"member_{i+1}\" for i in range(members_df.shape[1])]\n",
    "members_df['member_2'] = members_df['member_2'].str.replace(' ', '')\n",
    "members_df = members_df.replace(user_id_to_username)\n",
    "\n",
    "# Concatenate the original DataFrame with the new columns\n",
    "dms_df = pd.concat([dms_df, members_df], axis=1)\n",
    "\n",
    "# # rename user id with username\n",
    "# for column in dms_df.columns:\n",
    "#     if column.startswith('member_'):\n",
    "#         dms_df[column] = dms_df[column].replace(user_id_to_username)\n",
    "\n",
    "# Display the modified DataFrame\n",
    "dms_df.to_csv('../processed_data/dms_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from fastapi import FastAPI, UploadFile\n",
    "\n",
    "# Establish a connection to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"your_database_name\",\n",
    "    user=\"your_username\",\n",
    "    password=\"your_password\"\n",
    ")\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/process_zip\")\n",
    "async def process_zip(file: UploadFile):\n",
    "    # Specify the directory to extract the zip file to\n",
    "    extract_directory = '../data4/raw/'\n",
    "\n",
    "    # Save the zip file\n",
    "    zip_file_path = os.path.join(extract_directory, file.filename)\n",
    "    with open(zip_file_path, 'wb') as f:\n",
    "        f.write(await file.read())\n",
    "\n",
    "    # Extract the zip file\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_directory)\n",
    "\n",
    "    # Get a list of JSON files in the directory\n",
    "    json_files = [os.path.join(root, f) for root, _, files in os.walk(extract_directory) for f in files if f.endswith('.json')]\n",
    "\n",
    "    # Process each JSON file and convert it to a Pandas DataFrame\n",
    "    df_list = []\n",
    "    for json_file in json_files:\n",
    "        with open(json_file, encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        df = pd.json_normalize(data)\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Save the DataFrame to the PostgreSQL database\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"CREATE TABLE IF NOT EXISTS processed_files (id SERIAL PRIMARY KEY, file_name VARCHAR, file_data JSONB)\")\n",
    "    for index, row in df.iterrows():\n",
    "        file_name = row['file_name']\n",
    "        file_data = row.to_json()\n",
    "        cursor.execute(\"INSERT INTO processed_files (file_name, file_data) VALUES (%s, %s)\", (file_name, file_data))\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "\n",
    "    return {\"message\": \"Zip file processed successfully\"}\n",
    "\n",
    "@app.get(\"/create_user_mapping\")\n",
    "async def create_user_mapping():\n",
    "    # Read the users.csv file\n",
    "    csv_file_path = '../csv/users.csv'\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Create a dictionary for mapping user_id with username\n",
    "    user_mapping = df.set_index('user_id')['username'].to_dict()\n",
    "\n",
    "    # Save the user_mapping to the PostgreSQL database\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"CREATE TABLE IF NOT EXISTS user_mapping (user_id VARCHAR PRIMARY KEY, username VARCHAR)\")\n",
    "    for user_id, username in user_mapping.items():\n",
    "        cursor.execute(\"INSERT INTO user_mapping (user_id, username) VALUES (%s, %s)\", (user_id, username))\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "\n",
    "    return {\"message\": \"User mapping created successfully\"}\n",
    "\n",
    "@app.get(\"/create_channel_info\")\n",
    "async def create_channel_info():\n",
    "    # Read the channels.csv file\n",
    "    csv_file_path = '../csv/channels.csv'\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Create a new DataFrame with channel information\n",
    "    channel_info_df = pd.DataFrame(columns=['channel_name', 'members', 'type'])\n",
    "\n",
    "    # Iterate through each channel\n",
    "    for index, row in df.iterrows():\n",
    "        channel_name = row['channel_name']\n",
    "        members = row['members']\n",
    "        channel_type = row['type']\n",
    "\n",
    "        # Split the members string into a list\n",
    "        members_list = members.split(',')\n",
    "\n",
    "        # Create a new row for each member in the channel\n",
    "        for member in members_list:\n",
    "            channel_info_df = channel_info_df.append({'channel_name': channel_name, 'members': member, 'type': channel_type}, ignore_index=True)\n",
    "\n",
    "    # Save the channel information to the PostgreSQL database\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"CREATE TABLE IF NOT EXISTS channel_info (channel_name VARCHAR, members VARCHAR, type VARCHAR)\")\n",
    "    for index, row in channel_info_df.iterrows():\n",
    "        channel_name = row['channel_name']\n",
    "        members = row['members']\n",
    "        channel_type = row['type']\n",
    "        cursor.execute(\"INSERT INTO channel_info (channel_name, members, type) VALUES (%s, %s, %s)\", (channel_name, members, channel_type))\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "\n",
    "    return {\"message\": \"Channel information created successfully\"}\n",
    "\n",
    "@app.get(\"/get_single_user_messages\")\n",
    "async def get_single_user_messages(user_id: str):\n",
    "    # Read the messages.csv file\n",
    "    csv_file_path = '../csv/messages.csv'\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Filter the messages for the specified user_id\n",
    "    filtered_df = df[df['user_id'] == user_id]\n",
    "\n",
    "    # Save the filtered messages to the PostgreSQL database\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"CREATE TABLE IF NOT EXISTS single_user_messages (user_id VARCHAR, message VARCHAR)\")\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        message = row['message']\n",
    "        cursor.execute(\"INSERT INTO single_user_messages (user_id, message) VALUES (%s, %s)\", (user_id, message))\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "\n",
    "    return {\"message\": \"Single user messages created successfully\"}\n",
    "\n",
    "@app.get(\"/get_two_user_messages\")\n",
    "async def get_two_user_messages(user_id1: str, user_id2: str):\n",
    "    # Read the messages.csv file\n",
    "    csv_file_path = '../csv/messages.csv'\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Filter the messages for the specified user_id1 and user_id2\n",
    "    filtered_df = df[(df['user_id'] == user_id1) | (df['user_id'] == user_id2)]\n",
    "\n",
    "    # Save the filtered messages to the PostgreSQL database\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"CREATE TABLE IF NOT EXISTS two_user_messages (user_id VARCHAR, message VARCHAR)\")\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        message = row['message']\n",
    "        cursor.execute(\"INSERT INTO two_user_messages (user_id, message) VALUES (%s, %s)\", (user_id, message))\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "\n",
    "    return {\"message\": \"Two user messages created successfully\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write basic code to connect with postgresql database\n",
    "import psycopg2\n",
    "\n",
    "# Establish a connection to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"kentron\",\n",
    "    port = 5433,\n",
    "    user=\"postgres\",\n",
    "    password=\"Kingfr@ncesco015\"\n",
    ")\n",
    "\n",
    "# Create a cursor\n",
    "# cursor = conn.cursor()\n",
    "\n",
    "# # Execute a query\n",
    "# cursor.execute(\"SELECT * FROM users\")\n",
    "\n",
    "# # Retrieve query results\n",
    "# records = cursor.fetchall()\n",
    "\n",
    "# # Print the records\n",
    "# for record in records:\n",
    "#     print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code to create a table users with sample data and then retrieve the data and the delete the table\n",
    "\n",
    "# Create a cursor\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Execute a query\n",
    "cursor.execute(\"CREATE TABLE IF NOT EXISTS users (id SERIAL PRIMARY KEY, name VARCHAR, age INTEGER)\")\n",
    "\n",
    "# Insert a record\n",
    "# cursor.execute(\"INSERT INTO users (name, age) VALUES (%s, %s)\", (\"John\", 25))\n",
    "\n",
    "# Retrieve query results\n",
    "cursor.execute(\"SELECT * FROM users\")\n",
    "\n",
    "# Retrieve query results\n",
    "records = cursor.fetchall()\n",
    "\n",
    "# Print the records\n",
    "for record in records:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a record\n",
    "cursor.execute(\"DELETE FROM users WHERE name = %s\", (\"John\",))\n",
    "\n",
    "# Drop table\n",
    "cursor.execute(\"DROP TABLE users\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<connection object at 0x0000022680B53BF0; dsn: 'user=postgres password=xxx dbname=kentron host=localhost port=5433', closed: 1>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/extract_and_convert\")\n",
    "async def extract_and_convert(file: UploadFile):\n",
    "    # Specify the directory to extract the zip file to\n",
    "    extract_directory = '../data4/raw/'\n",
    "\n",
    "    # Save the zip file\n",
    "    zip_file_path = os.path.join(extract_directory, file.filename)\n",
    "    with open(zip_file_path, 'wb') as f:\n",
    "        f.write(await file.read())\n",
    "\n",
    "    # Extract the zip file\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_directory)\n",
    "\n",
    "    # Get a list of JSON files in the directory\n",
    "    json_files = [os.path.join(root, f) for root, _, files in os.walk(extract_directory) for f in files if f.endswith('.json')]\n",
    "\n",
    "    # Process each JSON file and convert it to a Pandas DataFrame\n",
    "    df_list = []\n",
    "    for json_file in json_files:\n",
    "        with open(json_file, encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        df = pd.json_normalize(data)\n",
    "        df_list.append(df)\n",
    "\n",
    "        # Convert JSON file to CSV\n",
    "        csv_file = os.path.splitext(json_file)[0] + '.csv'\n",
    "        df.to_csv(csv_file, index=False)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Read the channels.csv file\n",
    "    channels_df = pd.read_csv('../data4/raw/channels.csv')\n",
    "\n",
    "    # Select the desired columns\n",
    "    channel_info_df = channels_df[['id', 'name', 'is_general', 'creator', 'created', 'members', 'is_archived']]\n",
    "\n",
    "    # Specify the directory to save the channel_info.csv file\n",
    "    save_directory = '../data4/processed/'\n",
    "\n",
    "    # Save the channel_info DataFrame to a CSV file\n",
    "    channel_info_df.to_csv(os.path.join(save_directory, 'channel_info.csv'), index=False)\n",
    "\n",
    "    # Read the users.csv file\n",
    "    users_df = pd.read_csv('../data4/raw/users.csv')\n",
    "\n",
    "    # Select the desired columns\n",
    "    users_info_df = users_df[['id', 'team_id', 'name', 'deleted', 'is_bot', 'is_app_user', 'updated']]\n",
    "\n",
    "    # Save the users_info DataFrame to a CSV file\n",
    "    users_info_df.to_csv(os.path.join(save_directory, 'users_info.csv'), index=False)\n",
    "\n",
    "    # Create a dictionary to map id and name\n",
    "    id_name_mapping = dict(zip(users_info_df['id'], users_info_df['name']))\n",
    "\n",
    "    # Define the directory path\n",
    "    directory_path = \"../data4/raw/\"\n",
    "    column_names = [\"user\", \"text\", \"type\", \"ts\"]\n",
    "\n",
    "    # Create an empty dataframe\n",
    "    df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "    # Loop through all subdirectories starting with \"D05\"\n",
    "    for subdir, dirs, files in os.walk(directory_path):\n",
    "        if subdir.startswith(directory_path + \"D05\"):\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    # Read the csv file\n",
    "                    file_path = os.path.join(subdir, file)\n",
    "                    temp_df = pd.read_csv(file_path)\n",
    "                    \n",
    "                    # Check if all required columns are present\n",
    "                    if all(col in temp_df.columns for col in column_names):\n",
    "                        # Append the dataframe to the main dataframe\n",
    "                        df = df.append(temp_df[column_names], ignore_index=True)\n",
    "\n",
    "    df['ts'] = pd.to_datetime(df['ts'], unit='s')\n",
    "\n",
    "    # Save the dataframe to a csv file\n",
    "    df.to_csv(\"../data4/processed/merged_msg.csv\", index=False)\n",
    "\n",
    "    return {\"message\": \"Files extracted and converted successfully.\", \"id_name_mapping\": id_name_mapping}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Files added to PostgreSQL database successfully.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "# Establish a connection to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"kentron\",\n",
    "    user=\"postgres\",\n",
    "    password=\"Kingfr@ncesco015\"\n",
    ")\n",
    "\n",
    "# Create a cursor\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Specify the directory containing the processed files\n",
    "processed_directory = '../data4/processed/'\n",
    "\n",
    "# Get a list of CSV files in the directory\n",
    "csv_files = [os.path.join(processed_directory, f) for f in os.listdir(processed_directory) if f.endswith('.csv')]\n",
    "\n",
    "# Iterate through each CSV file\n",
    "for csv_file in csv_files:\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Get the table name from the file name\n",
    "    table_name = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "\n",
    "    # Create the table in the database if it doesn't exist\n",
    "    create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} (\"\n",
    "    for column in df.columns:\n",
    "        column_type = df[column].dtype\n",
    "        if column_type == 'int64':\n",
    "            create_table_query += f\"{column} INTEGER,\"\n",
    "        elif column_type == 'float64':\n",
    "            create_table_query += f\"{column} FLOAT,\"\n",
    "        elif column_type == 'bool':\n",
    "            create_table_query += f\"{column} BOOLEAN,\"\n",
    "        else:\n",
    "            create_table_query += f'\"{column}\" VARCHAR,'  # Change the column name to be enclosed in double quotes\n",
    "    create_table_query = create_table_query.rstrip(',') + \")\"\n",
    "    cursor.execute(create_table_query)\n",
    "\n",
    "        # Replace NaN values with None\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "\n",
    "    # Insert the data into the table\n",
    "    for index, row in df.iterrows():\n",
    "        insert_query = f\"INSERT INTO {table_name} VALUES (\"\n",
    "        for value in row.values:\n",
    "            if value is None:\n",
    "                value = 'None'  # Replace None with the string 'None'\n",
    "            elif isinstance(value, str):\n",
    "                # Escape single quotes in the string\n",
    "                value = value.replace(\"'\", \"''\")\n",
    "            insert_query += f\"'{value}',\"\n",
    "        insert_query = insert_query.rstrip(',') + \")\"\n",
    "        cursor.execute(insert_query)\n",
    "\n",
    "# Commit the changes\n",
    "conn.commit()\n",
    "\n",
    "# Return the result\n",
    "{\"message\": \"Files added to PostgreSQL database successfully.\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
